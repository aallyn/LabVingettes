---
title: "RMarkdownExample"
author: "Andrew Allyn"
date: "5/9/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
This is a short RMarkdown document to try to demonstrate a workflow that:
1. Uses the power of Git and GitHub for version controlled collaborative coding
2. Uses the power of Box for file backup and sharing files between lab members and the research department
3. Uses RMarkdown to nicely blend code, text and figures to create temporary results files that can be easily reviewed by lab members

## Workflow steps -- Your own project
1. I added some new folders on Box within my personal folder ("ExampleSharedData", "ExampleSharedFunctions", and "GitHub"). This was necessary since there isn't currently a Research Folder that would have "Data" and "Functions"
1. I created a new repo on GitHub.com called "LabVingettes" and initated it with a .gitignore for R
2. After doing the set up on GitHub online, I then cloned the repository by copying the https link and set up a new RStudio Git version controlled project called "LabVingettes". A key piece here, when I did this, I actually put the project into a "GitHub" folder that I created within my personal ("Andrew Allyn") folder on Box. Moving forward, once we have the lab folder on Box, I'd imagine that this process would actually be done within a project folder in the lab folder on Box. 
3. After setting up this RProject, it creates a LabVingettes folder in the GitHub folder and inside the LabVingettes folder are a couple files as well as the LabVingettes.RProj file.
4. Within the LabVingettes folder, I added three new folders: Code, Data, Temp Results. I then also opened and edited the .gitignore file to ignore the Data and Temp Results folders. 
5. I wrote a RMarkdown document which uses data from the LabVingettes project as well as data and functions from the ExampleShared folders.
6. After completing an example RMarkdown document, I staged, committed and pushed the RMarkdown code to GitHub.
7. I opened a new script to render the RMarkdown document as this was the only way I could think of to save the rendered document into a new folder (Temp Results), while keeping the RMarkdown code in the Code folder.
8. I staged, committed and pushed the render code to GitHub
9. I ran the render code and looked in the Temp Results file to see the out -- an html file build from the RMarkdown document and a .jpg plot.

## Start data exploration
A few preliminary things to start. First, we are going to source some functions that are in the ExampleSharedFunctions and some data in the ExampleSharedData folder. Importantly, this folder is currenty in my personal directory **and for now, you will need to update the "user name" in the code chunk below**. The idea would be that these folders would exist in the Research folder (e.g., Box-Research-Data or Box-Research-Functions). Given that, only the very beginning of the file path should change depending on the operating system. I imagine this would need to be at the beginning of all codes? *Is there a better way to do this??*
```{r}
# Detect the operating system
os.use<- .Platform$OS.type

# Set path to shared folders
user.name<- "Andrew Allyn/" 
computer.name<- "" # Needed for PC users
shared.path<- switch(os.use, 
                     "unix" = paste("~/Box/", user.name, sep = ""),
                     "windows" = paste("C:/Users/", computer.name, "/Box/", user.name, sep = ""))

# Source functions -- this is a little clumsy with the output text, but works
shared.funcs<- list.files(paste(shared.path, "ExampleSharedFunctions", sep = ""), full.names = TRUE)
sapply(shared.funcs, source)

# Load libraries
libraries.needed<- c("tidyverse", "here", "sf", "viridis", "SDMTools")
library_check(libraries.needed)
```

Now, let's see about reading in some example data (NOAA NEFSC bottom trawl survey data for Altantic cod), which is stored within the project data folder. Maybe using the here library? And then reading in a shapefile, which would be located in some shared folder.
```{r}
# Reading in cod data -- would want to try this with someone else's copy of this folder?
cod.dat<- read_csv(here("Data", "NOAANEFSCBottomTrawl_CodObs.csv"))
summary(cod.dat)

# Let's also read in the NELME shapefile, which is located in a different folder
# Option one...
nelme<- st_read(paste(shared.path, "ExampleSharedData/Shapefiles/NELME_sf.shp", sep = ""))
ggplot() +
  geom_sf(data = st_simplify(nelme, dTolerance = 0.001))

# That worked -- I don't think we can use the "here" piece because "here" is going to be looking inside this RProject and its corresponding folders. The ExampleSharedData folder, however, will be located outside this RProject.
```

Alright, so seemingly we are able to read in some data that is within the RProject Data folder and we are also able to read in a shared shapefile of the Northeast Large Marine Ecosystem. Next, let's make a quick plot of Cod Center of Gravity and save the figure to the Temp Results folder in this project...
```{r}
# The center of gravity will be calculated for each year and season independently. So, looking at a replicated process and either loops OR using new tidyverse functions to nest the data and then map a function to each row of the nested data. 

# Define season based on survey month
cod.dat$SEASON<- ifelse(cod.dat$EST_MONTH >= 2 & cod.dat$EST_MONTH <= 5, "SPRING", "FALL")
                        
# Nested dataframe by year and season
cod.nest<- cod.dat %>%
  group_by(., EST_YEAR, SEASON) %>%
  nest(., .key = "ObsData")

# Check it out -- all the data for a given year and season is collapsed and lives in the "ObsData" column
cod.nest 

# With that set up, let's write a little function hat uses each row of the ObsData to calculate the center of gravity (using SDMTools COGravity function). 
cog_map_func<- function(df){
  df.temp<- df %>%
    drop_na(DECDEG_BEGLON, DECDEG_BEGLAT, BIOMASS.MOD)
  cog.temp<- COGravity(x = df.temp$DECDEG_BEGLON, y = df.temp$DECDEG_BEGLAT, z = df.temp$BIOMASS.MOD)
  cog.out<- data.frame("Long" = cog.temp[1], "Lat" = cog.temp[3])
  return(cog.out)
}

# Now, map this function to each of the ObsData rows, creating a new COG output column
cod.nest<- cod.nest %>%
  mutate(., "COG" = map(ObsData, cog_map_func))

# Alright, now if we have a dataset with EST_YEAR, SEASON, COGx and COGy, we might be able to look at how COG has changed over time...
cod.plot.dat<- cod.nest %>%
  dplyr::select(., EST_YEAR, SEASON, COG) %>%
  unnest()

# Plot, gradient applied to year and faceted by season. For some reason, ggplot isn't liking the geom_sf and then adding points...
nelme.poly<- fortify(as(st_simplify(nelme, dTolerance = 0.001), "Spatial"))
cod.cog.plot<- ggplot() +
  geom_polygon(data = nelme.poly, aes(x = long, y = lat, group = group), color = "black", fill = NA) +
  geom_point(data = cod.plot.dat, aes(x = Long, y = Lat, color = EST_YEAR), alpha = 0.5) +
  scale_color_viridis() +
  theme_bw() +
  coord_map() +
  facet_wrap(~SEASON)

# Save it as a jpg and print it to the Markdown file
ggsave(here("Temp Results", "ExampleCodCOGPlot.jpg"), cod.cog.plot)

# Print it to Markdown file
cod.cog.plot
```

## Work flow steps -- someone else's project
A quick status update on what this has allowed us to do. We now have:
- version controlled, open source coding (using Git/GitHub) using RStudio projects
- code that is backed up and synced with Box
- code that reads in data and sources functions from shared file folders on Box
- analysis workflows that can deployed across different users and operating systems given only minor adjustments to two lines of the code

The last major hurdle seems to be dealing with sharing these analysis workflows or a situation where a lab member leaves the lab. In the past, this has been a bit of a nightmare and usually includes some crazy long word document outlining where the different scripts/data/results are located. Inevitably, the code also requires extensive modification for someone else to use. So, how do we get around that?

I think, with the current set up we should be able to also overcome this hurdle. The key piece, though, is that when a new GitHub repo is cloned to make the RStudio Project, we need to make sure to iniate the project **on Box in the shared lab folder.** If we do this, and make sure to use the path generation lines detailed here to create file paths to any shared folders, and use the here library to source things associated with the project, we should be all set. When a project is completed, or if someone leaves, all of the work (Code, Data, Documents, Figures and Tables) will live in the one project folder on Box, also linked to a GitHub repository. 
## Remaining questions
- How does this all work with collaborative coding? Would you create a fork/clone in your own personal file system and then work from there? How much would that screw up the file paths?