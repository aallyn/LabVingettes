---
title: "Basic spatial data manipulation and operations"
author: "Andrew Allyn"
date: "6/14/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
This is a *really* short example of a few things you can do with spatial data using the sf/tidyverse/raster libraries. It is by no means an exhaustive tutorial and really only covers some of the things I find myself doing most frequently. These tasks include:

* Reading in spatial data
* Subsetting spatial data
* Making maps
* Basic spatial data operations (points in polygons, distances between spatial features, extracting spatial data at sample point locations)

For more detailed guides, check out the following links:

* [SF library vingettes](https://r-spatial.github.io/sf/articles/sf1.html) -- make sure to look through the different articles!
* [Robin Lovelace's excellent, free, online book!](https://bookdown.org/robinlovelace/geocompr/) 

## Spatial data manipulation and basic operations
### Before we get rolling...
You will need a few different libraries to run everything, including: raster, tidyverse, sf, and viridis. I *think* that we should be able to download (if needed) and load these just by running the following chuck. If you run into issues, though, feel free to adapt this document and download/install libraries as you normally would.
```{r, results = "hide"}
source("https://raw.githubusercontent.com/GMRI-SEL/LabFunctionsandCode/master/GenerateSharedPathsScript.R")

# Source "library_check" helper function to install/load required libraries
source(paste(lab.func.path, "GeneralHelpers.R", sep = ""))
library_check(c("raster", "tidyverse", "sf", "viridis", "here"))
```

### Loading in a spatial dataset that is in "shapefile" format, examining it, and making a map
Alright, the first thing we are going to do is load in a spatial shapefile -- in this case North Carolina counties, which ships with the sf library. This is done using the "st_read" function in the sf library. As you will see with the "class(nc)" bit, we are creating a "sf" object, which is short for "simple features." One way of thinking about this is that an sf object consists of two things: 1) A geometry (point, line, polygon, etc) and then 2) A data frame, which holds relevant information about each geometry.
```{r}
nc <- st_read(system.file("shape/nc.shp", package="sf"))
glimpse(nc)
class(nc)
```

One thing to always keep in mind with spatial data is its coordinate reference system (CRS). Entire books are written about this stuff and how we can display data that occurs over the earth's curved surface on flat 2-D paper. I'm not going to go into much depth on the topic of datums, ellipsoids and projections, except to cover the basics. If you are really interested in getting into the weeds, have a look [here to start](https://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/geographic-vs-projected-coordinate-reference-systems-UTM/).

Generally, we are dealing with either a geographic CRS or a projected CRS. One of the most common examples of a geographic CRS is the latitude/longitude decimal degrees CRS that uses the WGS84 datum. This is *most likely* what comes to mind when we think of spatial locations: latitude -90:90 and longitude -180:180. The lat/long geographic CRS is best suited for displaying data that spans the entire globe. In my experience, it also tends to match how I *think* things should look regionally and locally. A projected CRS, on the other hand, is probably a new concept. Generally, projected CRSs are best for regional or local data. One example of a projected CRS is the Universal Transverse Mercator projection, which subdivides the globe into zones, numbered 0-60 (equivalent to longitude) and regions (north and south). Locally, for instance, we are in zone 19N. The coordinates are in easting and northings, which are measured in meters. The Northeast Large Marine Ecosystem would include both zone 18N and zone 19N. 

I mention this CRS business just to highlight two things:

1. To display information on a map from two different data sources, OR, to use two (or more) data sources in a function, **they need to have the same CRS!** 
2. When working with distances, it is usually better to calculate distances using data in a projected CRS than in a geographic CRS. *In the distance calculation below, I didn't do this*

```{r}
# Checking the CRS of a spatial object
st_crs(nc)

# Projecting a geographic CRS -- we get the "epsg" code for UTM Zone 19N by searching the spatial reference website: https://spatialreference.org
nc.utm<- st_transform(nc, crs = "+init=epsg:2960")
par(mfrow = c(1,2))
plot(st_geometry(nc), main = "Geographic Lat/Long CRS")
plot(st_geometry(nc.utm), main = "Projected UTM Zone 19N CRS")
```

Once read into R, we can use ggplot and the geom_sf capability to make a quick map.
```{r}
# Make a quick map
nc.map<- ggplot() +
  geom_sf(data = nc) + 
  coord_sf() +
  theme_bw()
nc.map
```

### Spatial data manipulation
Now that we've got a spatial dataset into R (as an sf object), we are usually going to want to manipulate it in some way. For the most part, any type of manipulation we'd do with normal data can be done with the spatial sf data using the same functions. As a quick example, here we are going to filter the North Carolina data and create a new spatial sf dataset, which only includes a subset of all the counties: Ashe and Currituck.
```{r}
## Subsetting a spatial feature 
name.keep<- c("Ashe", "Currituck")
nc.sub<- nc %>%
  filter(., NAME %in% name.keep)
nc.sub

nc.map2<- nc.map +
  geom_sf(data = nc.sub, fill = "#1b9e77") +
  coord_sf() +
  theme_bw()
nc.map2
```

### Spatial data operations
A lot of the spatial data I deal with (and I think most folks deal with on a regular basis) consists of spatial point location data. Given these data, we are usually interested in figuring out where certain points occur (point in polygons), how far points are from another spatial feature of interest (point to feature distances), and extracting information about the point location using outside data sources. Though this seems to be the operations I repeat over and over again, there are many, many more possibilities. And, they've likely already been implemented by someone else. So, always check in with google first.

To demonstrate these basic operations, we will first create some spatial points.
```{r}
## Creating some sample points within a region
samp.pts<- st_sample(nc, size = 400, type = "random")
nc.map3<- nc.map2 +
  geom_sf(data = samp.pts, color = "#7570b3") +
  coord_sf() +
  theme_bw()
nc.map3
```

Now that we have some example sample locations. We can explore some basic operations -- point in polygon, and distances between spatial feature objects.
```{r}
## Determining which county each point falls into. I have no clue why you need the st_sf other than it is changing class(samp.pts) from sfc_POINT to "sf"
pts.county<- st_join(st_sf(samp.pts), nc)
glimpse(samp.pts)
glimpse(pts.county) # Note added county information!

## Subsetting a spatial points object and keeping points that are within certain polygons?
# Option 1 with original sample points. Again, annoying to do st_sf, but only way it works! I'm also not sure about the "lengths > 0" bit. I was expecting you could just filter(st_within), but that doesn't work. The st_within (or any of the other binary st type information) returns a vector and this is basically pulling out elements of the vector that have lengths > 0, signaling a point is one of the polygons. 
pts.sub<- st_sf(samp.pts) %>%
  dplyr::filter(lengths(st_within(x = ., y = nc.sub)) > 0)

nc.map4<- nc.map3 +
  geom_sf(data = pts.sub, color = "#d95f02") +
  coord_sf() +
  theme_bw()
nc.map4

# Option 2, we can just filter the pts.county object
pts.sub2<- pts.county %>%
  filter(., NAME  %in% name.keep)
pts.sub2

## Distance from pts to a specific feature, maybe to the captial
cap.loc<- data.frame("X" = -78.6382, "Y" = 35.7796, "Label" = "Raleigh, NC")
cap.sf<- sf::st_as_sf(cap.loc, coords = c("X", "Y"), crs = st_crs(nc))
nc.capmap<- nc.map +
  geom_sf(data = cap.sf, pch = 8, size = 3, color = "#d95f02") +
  coord_sf() +
  theme_bw()
nc.capmap

pts.county<- pts.county %>%
  mutate(., "DISTANCE.TO.RALEIGH" = st_distance(pts.county, cap.sf))

# Plot closest and farthest
pts.closest<- pts.county %>%
  filter(DISTANCE.TO.RALEIGH == min(DISTANCE.TO.RALEIGH, na.rm = TRUE))%>%
  mutate("Plot.Label" = round(DISTANCE.TO.RALEIGH, 0))
pts.furthest<- pts.county %>%
  filter(DISTANCE.TO.RALEIGH == max(DISTANCE.TO.RALEIGH, na.rm = TRUE)) %>%
  mutate("Plot.Label" = round(DISTANCE.TO.RALEIGH, 0))

nc.capmap2<- nc.capmap +
  geom_sf(data = pts.closest, pch = 21, size = 3, fill = "#1b9e77") +
  geom_sf(data = pts.furthest, pch = 21, size = 3, fill = "#7570b3") +
  coord_sf() +
  theme_bw()
nc.capmap2
```

Along with st_within there is also st_intersects, st_crosses, st_contains, st_touches, st_is_within_distance, etc. Just have a look at ?st_within for more ideas.

Finally, extracting variable information from another data source (in this case a spatial surface or raster) at the sample point locations. 
```{r}
## Need some type of raster data or something to extract from. Raster library has some data availabile (climate, elevation, etc) at a few different resolutions
clim.rast<- getData("worldclim", var = "tmean", res = 10, path = here("Data"))
#str(clim.rast) # Not helpful
print(clim.rast) # That's better
plot(clim.rast)

# Get annual average mean temp
temp.avg<- mean(clim.rast)
plot(temp.avg)

# Temps are Deg C*10?
temp.avg2<- temp.avg/10
plot(temp.avg2)

# Alright, that will work. 
# A lot of data...focus on what we want?
# Create an extent polygon -- got to be a better way to do this.
nc.bbox<- st_bbox(nc)
x.bbox<- as.numeric(c(nc.bbox[1], nc.bbox[3]))
y.bbox<- as.numeric(c(nc.bbox[2], nc.bbox[4]))
extent.nc<- raster::extent(c(x.bbox, y.bbox))

# Crop temperature
nc.temp<- raster::crop(temp.avg2, extent.nc)
plot(nc.temp)

# Plot it?
nc.temp.df<- as.data.frame(nc.temp, xy = TRUE)
nc.temp.plot<- ggplot() + 
  geom_raster(data = nc.temp.df, aes(x = x, y = y, fill = layer)) + 
  scale_fill_viridis() + 
  geom_sf(data = nc, fill = NA) + 
  coord_sf() +
  theme_bw()
nc.temp.plot

# Close. Really just want the temp data within the boundaries of the state though, not the bounding box.
nc.temp2<- raster::mask(temp.avg2, nc)
nc.temp2.df<- as.data.frame(nc.temp2, xy = TRUE)
nc.temp.plot2<- ggplot() + 
  geom_raster(data = nc.temp2.df, aes(x = x, y = y, fill = layer), na.rm = TRUE) + 
  scale_fill_viridis(name = "Air Temperature", na.value = "white") + 
  geom_sf(data = nc, fill = NA) + 
  coord_sf(xlim = x.bbox, ylim = y.bbox) +
  theme_bw()
nc.temp.plot2

# Bit better, would be nicer with having raster exactly cropped to the shapefile and I'm sure there's a way to get that done. But, moving on for now.

## Extracting air temperature (nc.temp2) at point locations (pts.county). A note here, when using "y = .", I am simply passing the pts.county spatial points to the y argument in the raster extract function. 
pts.county<- pts.county %>%
  mutate("AIRTEMP" = raster::extract(x = nc.temp2, y = .))

# Did it work?
summary(pts.county$AIRTEMP)
```

The end, for now...


